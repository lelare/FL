{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flwr as fl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from alibi_detect.cd import MMDDrift\n",
    "from alibi_detect.cd.pytorch import preprocess_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed and device\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clients = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and preprocess CIFAR-10 dataset\n",
    "(all_x_train, all_y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "all_x_train, x_test = all_x_train / 255.0, x_test / 255.0 \n",
    "all_x_train = all_x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "all_y_train = all_y_train.astype('int64').reshape(-1,)\n",
    "y_test = y_test.astype('int64').reshape(-1,)\n",
    "\n",
    "x_train = all_x_train[0:int(len(all_x_train)*0.8)]\n",
    "y_train = all_y_train[0:int(len(all_y_train)*0.8)]\n",
    "\n",
    "x_val = all_x_train[int(len(all_x_train)*0.8):]\n",
    "y_val = all_y_train[int(len(all_y_train)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate federated clients (splitting the dataset)\n",
    "client_data = []\n",
    "\n",
    "for i in range(n_clients):\n",
    "    start = i * len(x_train) // n_clients\n",
    "    end = (i + 1) * len(x_train) // n_clients\n",
    "    client_data.append((x_train[start:end], y_train[start:end]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(client_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a global model (we had CNN in Task1/2 (class Net))\n",
    "\n",
    "encoding_dim = 32\n",
    "# define encoder\n",
    "global_model = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, 4, stride=2, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 128, 4, stride=2, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(128, 512, 4, stride=2, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(2048, encoding_dim)\n",
    ").to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_c(x):\n",
    "    return np.transpose(x.astype(np.float32), (0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMD detector on each client\n",
    "client_detectors = []\n",
    "for x_data, _ in client_data:\n",
    "    \n",
    "    # define preprocessing function\n",
    "    preprocess_fn = partial(preprocess_drift, model=global_model, device=device, batch_size=512)\n",
    "\n",
    "    X_ref = permute_c(x_data[0:200])\n",
    "    # initialise drift detector\n",
    "    detector = MMDDrift(X_ref, backend='pytorch', p_val=.05, \n",
    "                preprocess_fn=preprocess_fn, n_permutations=100)\n",
    "    client_detectors.append(detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model train\n",
    "def train(x_data, y_data, local_model, num_epochs=5):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(local_model.parameters())\n",
    "    local_model.train() \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for inputs, labels in zip(x_data, y_data):\n",
    "            inputs, labels = torch.from_numpy(inputs).to(device), torch.from_numpy(labels).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = local_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}, Loss: {epoch_loss / len(x_data)}')\n",
    "\n",
    "    return local_model\n",
    "\n",
    "# Moddel test\n",
    "def test(x_data, y_data, local_model):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    local_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in zip(x_data, y_data):\n",
    "            inputs, labels = torch.from_numpy(inputs).to(device), torch.from_numpy(labels).to(device)\n",
    "            outputs = local_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    loss /= len(x_data)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drift detection on client data\n",
    "def handle_client_drift(x_data, detector):\n",
    "    is_drift, metrics = detector.predict(permute_c(x_data))\n",
    "    if is_drift:\n",
    "        print(\"Drift detected on client data.\")\n",
    "        # local_model = train(x_data, y_data, local_model, num_epochs=5)\n",
    "    else:\n",
    "        print(\"No drift detected on client data. Continuing training.\")\n",
    "\n",
    "\n",
    "# Drift detection on aggregated data\n",
    "def handle_global_drift(aggregated_data, detector):\n",
    "    is_drift, metrics = detector.predict(permute_c(aggregated_data))\n",
    "    print(metrics) # I think we can get p-value from metrics\n",
    "    if is_drift:\n",
    "        print(\"Drift detected on aggregated data. Updating global model.\")\n",
    "        # Q?: Should I update the global model here?\n",
    "    else:\n",
    "        print(\"No drift detected on aggregated data. Continuing training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, client_data, local_model, client_detector):\n",
    "        self.client_data = client_data\n",
    "        self.local_model = local_model\n",
    "        self.client_detector = client_detector\n",
    "\n",
    "    def get_parameters(self):\n",
    "        # Return the current model parameters\n",
    "        return self.local_model.state_dict()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        # Train the local model after updating it with the given parameters\n",
    "        self.local_model.load_state_dict(parameters)\n",
    "        self.local_model = train(self.client_data[0], self.client_data[1], self.local_model, num_epochs=5)\n",
    "        # Perform local training with client_data and drift detection\n",
    "        handle_client_drift(self.client_data[0], self.client_detector, self.local_model)\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        # Perform the evaluation of the model after updating it with the given\n",
    "        # parameters. Returns the loss as a float, the length of the validation\n",
    "        # data, and a dict containing the accuracy\n",
    "        self.local_model.load_state_dict(parameters)\n",
    "        loss, accuracy = test(x_val, y_val, self.local_model)\n",
    "        # Q?: Can I run handle_global_drift here instead of test?\n",
    "        # Q?: Do I need test in general?\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_fn(cid: str, client_data=client_data) -> FlowerClient:\n",
    "    for x_data, y_data in client_data:\n",
    "        # Apply drift detection on client data\n",
    "        handle_client_drift(x_data, client_detectors[int(cid)])\n",
    "        \n",
    "        local_model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 512, 4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048, encoding_dim)\n",
    "        ).to(device)\n",
    "\n",
    "        # Train the local model\n",
    "        local_model = train(x_data, y_data, local_model, num_epochs=5)\n",
    "        \n",
    "        return FlowerClient(local_model, train_data=(x_data, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)\n",
    "client_resources = None\n",
    "if device == \"cuda\":\n",
    "    client_resources = {\"num_gpus\": 1, 'num_cpus': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svmem(total=8181506048, available=1909977088, percent=76.7, used=5284409344, free=1733263360, active=999755776, inactive=5134344192, buffers=25178112, cached=1138655232, shared=713265152, slab=146976768)\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "print(psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FedAvg strategy\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1.0,          # Sample 100% of available clients for training\n",
    "    fraction_evaluate=0.5,     # Sample 50% of available clients for evaluation\n",
    "    min_fit_clients=2,        # Never sample less than 10 clients for training\n",
    "    min_evaluate_clients=2,    # Never sample less than 5 clients for evaluation\n",
    "    min_available_clients=10,  # Wait until all 10 clients are available\n",
    "    evaluate_metrics_aggregation_fn=handle_global_drift\n",
    "    # Q?: should I call handle_global_drift here?\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-12-01 17:00:54,706 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=5, round_timeout=None)\n",
      "2023-12-01 17:00:59,786\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "INFO flwr 2023-12-01 17:01:02,318 | app.py:210 | Flower VCE: Ray initialized with resources: {'memory': 1840231220.0, 'object_store_memory': 920115609.0, 'CPU': 8.0, 'node:172.29.213.102': 1.0, 'node:__internal_head__': 1.0}\n",
      "INFO flwr 2023-12-01 17:01:02,322 | app.py:218 | No `client_resources` specified. Using minimal resources for clients.\n",
      "INFO flwr 2023-12-01 17:01:02,323 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
      "INFO flwr 2023-12-01 17:01:02,354 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 8 actors\n",
      "INFO flwr 2023-12-01 17:01:02,358 | server.py:89 | Initializing global parameters\n",
      "INFO flwr 2023-12-01 17:01:02,360 | server.py:276 | Requesting initial parameters from one random client\n",
      "\u001b[2m\u001b[36m(pid=23501)\u001b[0m 2023-12-01 17:01:05.422872: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[2m\u001b[36m(pid=23501)\u001b[0m 2023-12-01 17:01:05.481809: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "\u001b[2m\u001b[36m(pid=23501)\u001b[0m 2023-12-01 17:01:05.980764: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(pid=23501)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=23501)\u001b[0m 2023-12-01 17:01:08.381421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m   File \"/home/leyla/.local/lib/python3.8/site-packages/ray/_private/serialization.py\", line 387, in deserialize_objects\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m   File \"/home/leyla/.local/lib/python3.8/site-packages/ray/_private/serialization.py\", line 268, in _deserialize_object\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m   File \"/home/leyla/.local/lib/python3.8/site-packages/ray/_private/serialization.py\", line 223, in _deserialize_msgpack_data\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m   File \"/home/leyla/.local/lib/python3.8/site-packages/ray/_private/serialization.py\", line 211, in _deserialize_pickle5_data\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m     obj = pickle.loads(in_band, buffers=buffers)\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m   File \"/home/leyla/.local/lib/python3.8/site-packages/torch/storage.py\", line 337, in _load_from_bytes\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m     return torch.load(io.BytesIO(b))\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m   File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 1028, in load\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m   File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 1256, in _legacy_load\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m     result = unpickler.load()\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m   File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 1193, in persistent_load\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m     wrap_storage=restore_location(obj, location),\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m   File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 381, in default_restore_location\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m     result = fn(storage, location)\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m   File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 274, in _cuda_deserialize\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m     device = validate_cuda_device(location)\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m   File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 258, in validate_cuda_device\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m     raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=23501)\u001b[0m RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "\u001b[2m\u001b[36m(pid=23494)\u001b[0m 2023-12-01 17:01:05.422876: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=23494)\u001b[0m 2023-12-01 17:01:05.977942: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=23494)\u001b[0m 2023-12-01 17:01:05.980760: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=23494)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=23494)\u001b[0m 2023-12-01 17:01:08.381906: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "ERROR flwr 2023-12-01 17:01:18,805 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
      "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
      "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/ray/_private/worker.py\", line 2524, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError: \u001b[36mray::DefaultActor.run()\u001b[39m (pid=23501, ip=172.29.213.102, actor_id=ffef2b13993f446aa950e99e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7692797790>)\n",
      "  At least one of the input arguments for this task could not be computed:\n",
      "ray.exceptions.RaySystemError: System error: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/storage.py\", line 337, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 1028, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 1256, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 1193, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 381, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 274, in _cuda_deserialize\n",
      "    device = validate_cuda_device(location)\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 258, in validate_cuda_device\n",
      "    raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
      "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "\n",
      "ERROR flwr 2023-12-01 17:01:18,807 | ray_client_proxy.py:148 | \u001b[36mray::DefaultActor.run()\u001b[39m (pid=23501, ip=172.29.213.102, actor_id=ffef2b13993f446aa950e99e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7692797790>)\n",
      "  At least one of the input arguments for this task could not be computed:\n",
      "ray.exceptions.RaySystemError: System error: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/storage.py\", line 337, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 1028, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 1256, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 1193, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 381, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 274, in _cuda_deserialize\n",
      "    device = validate_cuda_device(location)\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 258, in validate_cuda_device\n",
      "    raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
      "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "ERROR flwr 2023-12-01 17:01:18,808 | app.py:294 | \u001b[36mray::DefaultActor.run()\u001b[39m (pid=23501, ip=172.29.213.102, actor_id=ffef2b13993f446aa950e99e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f7692797790>)\n",
      "  At least one of the input arguments for this task could not be computed:\n",
      "ray.exceptions.RaySystemError: System error: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/storage.py\", line 337, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 1028, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 1256, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 1193, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 381, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 274, in _cuda_deserialize\n",
      "    device = validate_cuda_device(location)\n",
      "  File \"/home/leyla/.local/lib/python3.8/site-packages/torch/serialization.py\", line 258, in validate_cuda_device\n",
      "    raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
      "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "ERROR flwr 2023-12-01 17:01:18,809 | app.py:295 | Your simulation crashed :(. This could be because of several reasons.The most common are: \n",
      "\t > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.\n",
      "\t > All the actors in your pool crashed. This could be because: \n",
      "\t\t - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 1, 'num_gpus': 0.0} is not enough for your workload). Use fewer concurrent actors. \n",
      "\t\t - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 1, 'num_gpus': 0.0}.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-12-01 17:01:59,731 E 23251 23251] (raylet) node_manager.cc:3084: 6 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 19dc9bbbc6dfc114c46e4cf275ca079c8eebcc23fade82c44398d8fe, IP: 172.29.213.102) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.29.213.102`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=n_clients,\n",
    "    config=fl.server.ServerConfig(num_rounds=5),\n",
    "    strategy=strategy,\n",
    "    client_resources=client_resources,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
